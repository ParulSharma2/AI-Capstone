{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da69e327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMILES DataFrame:\n",
      "          drug  chembl_id                                             smiles\n",
      "0  paracetamol  CHEMBL112                                 CC(=O)Nc1ccc(O)cc1\n",
      "1    ibuprofen  CHEMBL521                         CC(C)Cc1ccc(C(C)C(=O)O)cc1\n",
      "2    celecoxib  CHEMBL118  Cc1ccc(-c2cc(C(F)(F)F)nn2-c2ccc(S(N)(=O)=O)cc2...\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import requests       # to fetch data from the web API\n",
    "import pandas as pd   # to organize and store the data\n",
    "\n",
    "# Step 1: Define a list of drug names we are interested in\n",
    "drug_names = ['paracetamol', 'ibuprofen', 'celecoxib']\n",
    "\n",
    "# Step 2: Create an empty list to collect drug information\n",
    "drug_data = []\n",
    "\n",
    "# Step 3: Loop through each drug to query ChEMBL and retrieve SMILES\n",
    "for drug in drug_names:\n",
    "    # Use ChEMBL API to search for the drug and get its ChEMBL ID\n",
    "    url = f\"https://www.ebi.ac.uk/chembl/api/data/molecule/search?q={drug}\"\n",
    "    response = requests.get(url, headers={\"Accept\": \"application/json\"})\n",
    "\n",
    "    # If the search is successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        results = response.json()\n",
    "        # Get the first hit from the search result\n",
    "        if results['molecules']:\n",
    "            chembl_id = results['molecules'][0]['molecule_chembl_id']\n",
    "\n",
    "            # Use the ChEMBL ID to get molecule details (like SMILES)\n",
    "            mol_url = f\"https://www.ebi.ac.uk/chembl/api/data/molecule/{chembl_id}.json\"\n",
    "            mol_response = requests.get(mol_url)\n",
    "\n",
    "            if mol_response.status_code == 200:\n",
    "                mol_data = mol_response.json()\n",
    "\n",
    "                # Extract canonical SMILES string\n",
    "                smiles = mol_data.get('molecule_structures', {}).get('canonical_smiles', 'NA')\n",
    "\n",
    "                # Append results to the list\n",
    "                drug_data.append({'drug': drug, 'chembl_id': chembl_id, 'smiles': smiles})\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data for {drug}\")\n",
    "\n",
    "# Step 4: Convert collected data into a pandas DataFrame\n",
    "df_smiles = pd.DataFrame(drug_data)\n",
    "\n",
    "# Step 5: Display the results\n",
    "print(\"SMILES DataFrame:\")\n",
    "print(df_smiles)\n",
    "\n",
    "# Optional: Save to CSV\n",
    "df_smiles.to_csv(\"drug_smiles.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bc21f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sesión global con reintentos y User-Agent\n",
    "import requests, time\n",
    "from requests.adapters import HTTPAdapter\n",
    "try:\n",
    "    from urllib3.util.retry import Retry\n",
    "except Exception:\n",
    "    # Compatibilidad si cambia la ruta en tu versión\n",
    "    from urllib3.util import Retry  # type: ignore\n",
    "\n",
    "SESSION = requests.Session()\n",
    "SESSION.headers.update({\"User-Agent\": \"Sandra-Capstone/1.0\"})\n",
    "retries = Retry(\n",
    "    total=5,\n",
    "    connect=5,\n",
    "    read=5,\n",
    "    backoff_factor=0.6,\n",
    "    status_forcelist=[429, 500, 502, 503, 504],\n",
    "    allowed_methods=frozenset([\"GET\"])\n",
    ")\n",
    "adapter = HTTPAdapter(max_retries=retries)\n",
    "SESSION.mount(\"https://\", adapter)\n",
    "SESSION.mount(\"http://\", adapter)\n",
    "\n",
    "# Intenta usar el bundle de certificados si está presente\n",
    "VERIFY = True\n",
    "try:\n",
    "    import certifi\n",
    "    VERIFY = certifi.where()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "def get_fasta_seq(uniprot_id, timeout=15):\n",
    "    bases = [\n",
    "        f\"https://rest.uniprot.org/uniprotkb/{uniprot_id}.fasta\",\n",
    "        f\"https://www.uniprot.org/uniprotkb/{uniprot_id}.fasta\",\n",
    "        f\"https://legacy.uniprot.org/uniprotkb/{uniprot_id}.fasta\",\n",
    "    ]\n",
    "    last_err = None\n",
    "    for url in bases:\n",
    "        try:\n",
    "            r = SESSION.get(url, timeout=timeout, verify=VERIFY)\n",
    "            if r.status_code == 200 and r.text and \">\" in r.text:\n",
    "                return \"\".join([ln.strip() for ln in r.text.splitlines() if not ln.startswith(\">\")])\n",
    "        except requests.exceptions.SSLError as e:\n",
    "            last_err = e\n",
    "            time.sleep(0.8)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            last_err = e\n",
    "            time.sleep(0.8)\n",
    "    # Si no se pudo, devolvemos None; el pipeline usará un fallback one hot.\n",
    "    print(f\"Advertencia: no se pudo descargar FASTA de {uniprot_id}. Seguiremos con fallback. Último error: {last_err}\")\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a02e34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e488f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pairs shape: (790, 6)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scipy.sparse._sparsetools'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 183\u001b[39m\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    181\u001b[39m     feat_cols += [\u001b[33m\"\u001b[39m\u001b[33mprot_COX1\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mprot_COX2\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mensemble\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/HUMBER/Clinical Bioinformatics/2nd Semester/Capstone Project/AI-Capstone/.venv/lib/python3.13/site-packages/sklearn/__init__.py:73\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# `_distributor_init` allows distributors to run custom init code.\u001b[39;00m\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# For instance, for the Windows wheel, this is used to pre-load the\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# vcomp shared library runtime for OpenMP embedded in the sklearn/.libs\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401 E402\u001b[39;00m\n\u001b[32m     70\u001b[39m     __check_build,\n\u001b[32m     71\u001b[39m     _distributor_init,\n\u001b[32m     72\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clone  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_show_versions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m show_versions  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[32m     76\u001b[39m _submodules = [\n\u001b[32m     77\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcalibration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     78\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcluster\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    114\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcompose\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    115\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/HUMBER/Clinical Bioinformatics/2nd Semester/Capstone Project/AI-Capstone/.venv/lib/python3.13/site-packages/sklearn/base.py:20\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_metadata_requests\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _MetadataRequester, _routing_enabled\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_missing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_scalar_nan\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_param_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_repr_html\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ReprHTMLMixin, _HTMLDocumentationLinkMixin\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/HUMBER/Clinical Bioinformatics/2nd Semester/Capstone Project/AI-Capstone/.venv/lib/python3.13/site-packages/sklearn/utils/__init__.py:9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m metadata_routing\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_bunch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Bunch\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_chunking\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gen_batches, gen_even_slices\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Make _safe_indexing importable from here for backward compat as this particular\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# helper is considered semi-private and typically very useful for third-party\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# libraries that want to comply with scikit-learn's estimator API. In particular,\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# _safe_indexing was included in our public API documentation despite the leading\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# `_` in its name.\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_indexing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     17\u001b[39m     _safe_indexing,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m     18\u001b[39m     resample,\n\u001b[32m     19\u001b[39m     shuffle,\n\u001b[32m     20\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/HUMBER/Clinical Bioinformatics/2nd Semester/Capstone Project/AI-Capstone/.venv/lib/python3.13/site-packages/sklearn/utils/_chunking.py:11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_param_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Interval, validate_params\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mchunk_generator\u001b[39m(gen, chunksize):\n\u001b[32m     15\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Chunk generator, ``gen`` into lists of length ``chunksize``. The last\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[33;03m    chunk may have a length less than ``chunksize``.\"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/HUMBER/Clinical Bioinformatics/2nd Semester/Capstone Project/AI-Capstone/.venv/lib/python3.13/site-packages/sklearn/utils/_param_validation.py:14\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumbers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Integral, Real\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m csr_matrix, issparse\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvalidation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _is_arraylike_not_scalar\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/HUMBER/Clinical Bioinformatics/2nd Semester/Capstone Project/AI-Capstone/.venv/lib/python3.13/site-packages/scipy/sparse/__init__.py:305\u001b[39m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimportlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_importlib\u001b[39;00m\n\u001b[32m    304\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_base\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m--> \u001b[39m\u001b[32m305\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_csr\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m    306\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_csc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/HUMBER/Clinical Bioinformatics/2nd Semester/Capstone Project/AI-Capstone/.venv/lib/python3.13/site-packages/scipy/sparse/_csr.py:11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_matrix\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m spmatrix\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_base\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _spbase, sparray\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_sparsetools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (csr_tocsc, csr_tobsr, csr_count_blocks,\n\u001b[32m     12\u001b[39m                            get_csr_submatrix, csr_sample_values)\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_sputils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m upcast\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_compressed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _cs_matrix\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'scipy.sparse._sparsetools'"
     ]
    }
   ],
   "source": [
    "# Capstone COX1 vs COX2 - versión simple con reintentos SSL y fallback de proteína\n",
    "# Requisitos: requests, pandas, numpy, scikit-learn\n",
    "\n",
    "import requests, time, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from requests.adapters import HTTPAdapter\n",
    "try:\n",
    "    from urllib3.util.retry import Retry\n",
    "except Exception:\n",
    "    from urllib3.util import Retry  # type: ignore\n",
    "\n",
    "# 1) Sesión robusta\n",
    "SESSION = requests.Session()\n",
    "SESSION.headers.update({\"User-Agent\": \"Sandra-Capstone/1.0\"})\n",
    "retries = Retry(total=5, connect=5, read=5, backoff_factor=0.6,\n",
    "                status_forcelist=[429,500,502,503,504],\n",
    "                allowed_methods=frozenset([\"GET\"]))\n",
    "adapter = HTTPAdapter(max_retries=retries)\n",
    "SESSION.mount(\"https://\", adapter)\n",
    "SESSION.mount(\"http://\", adapter)\n",
    "\n",
    "VERIFY = True\n",
    "try:\n",
    "    import certifi\n",
    "    VERIFY = certifi.where()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "def http_get(url, params=None, timeout=20):\n",
    "    r = SESSION.get(url, params=params, headers={\"Accept\":\"application/json\"}, timeout=timeout, verify=VERIFY)\n",
    "    r.raise_for_status()\n",
    "    return r\n",
    "\n",
    "# 2) IDs y parámetros\n",
    "CHEMBL_COX1 = \"CHEMBL2095188\"\n",
    "CHEMBL_COX2 = \"CHEMBL2094253\"\n",
    "UNIPROT_COX1 = \"P23219\"\n",
    "UNIPROT_COX2 = \"P35354\"\n",
    "STD_TYPE = \"IC50\"\n",
    "STD_UNITS = \"nM\"\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# 3) UniProt FASTA con fallback\n",
    "def get_fasta_seq(uniprot_id, timeout=15):\n",
    "    bases = [\n",
    "        f\"https://rest.uniprot.org/uniprotkb/{uniprot_id}.fasta\",\n",
    "        f\"https://www.uniprot.org/uniprotkb/{uniprot_id}.fasta\",\n",
    "        f\"https://legacy.uniprot.org/uniprotkb/{uniprot_id}.fasta\",\n",
    "    ]\n",
    "    last_err = None\n",
    "    for url in bases:\n",
    "        try:\n",
    "            r = SESSION.get(url, timeout=timeout, verify=VERIFY)\n",
    "            if r.status_code == 200 and r.text and \">\" in r.text:\n",
    "                return \"\".join([ln.strip() for ln in r.text.splitlines() if not ln.startswith(\">\")])\n",
    "        except requests.exceptions.SSLError as e:\n",
    "            last_err = e; time.sleep(0.8)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            last_err = e; time.sleep(0.8)\n",
    "    print(f\"Advertencia: no se pudo descargar FASTA de {uniprot_id}. Fallback one hot. Último error: {last_err}\")\n",
    "    return None\n",
    "\n",
    "seq_cox1 = get_fasta_seq(UNIPROT_COX1)\n",
    "seq_cox2 = get_fasta_seq(UNIPROT_COX2)\n",
    "\n",
    "AA = list(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "def aa_comp(seq):\n",
    "    n = len(seq)\n",
    "    return np.array([seq.count(a)/n for a in AA], dtype=float)\n",
    "\n",
    "# Si no hay FASTA, usa one hot de proteína como fallback\n",
    "if seq_cox1 and seq_cox2:\n",
    "    aa_cox1 = aa_comp(seq_cox1)\n",
    "    aa_cox2 = aa_comp(seq_cox2)\n",
    "    PROT_FEATURES = \"aa\"   # usaremos composición AA\n",
    "else:\n",
    "    aa_cox1 = None\n",
    "    aa_cox2 = None\n",
    "    PROT_FEATURES = \"onehot\"  # usaremos one hot\n",
    "\n",
    "# 4) Actividades de ChEMBL (paginación simple + reintentos)\n",
    "def get_activities(target_id, max_pages=10, page_size=200):\n",
    "    base = \"https://www.ebi.ac.uk/chembl/api/data/activity.json\"\n",
    "    rows = []\n",
    "    for p in range(max_pages):\n",
    "        params = {\n",
    "            \"target_chembl_id\": target_id,\n",
    "            \"standard_type\": STD_TYPE,\n",
    "            \"standard_units\": STD_UNITS,\n",
    "            \"limit\": page_size,\n",
    "            \"offset\": p*page_size\n",
    "        }\n",
    "        try:\n",
    "            r = http_get(base, params=params, timeout=25)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Advertencia: fallo en página {p} para {target_id}: {e}\")\n",
    "            break\n",
    "        data = r.json().get(\"activities\", [])\n",
    "        if not data: break\n",
    "        rows.extend(data)\n",
    "        time.sleep(0.1)\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "df1 = get_activities(CHEMBL_COX1)\n",
    "df2 = get_activities(CHEMBL_COX2)\n",
    "\n",
    "# 5) Limpieza básica y etiqueta\n",
    "for df in (df1, df2):\n",
    "    df[\"standard_value\"] = pd.to_numeric(df[\"standard_value\"], errors=\"coerce\")\n",
    "\n",
    "def clean(df):\n",
    "    cols = [\"molecule_chembl_id\",\"target_chembl_id\",\"standard_value\",\"canonical_smiles\"]\n",
    "    for c in cols:\n",
    "        if c not in df.columns: df[c] = None\n",
    "    df = df.dropna(subset=[\"standard_value\"])\n",
    "    df = df[df[\"standard_value\"] > 0]\n",
    "    df = (df.groupby([\"molecule_chembl_id\",\"target_chembl_id\"], as_index=False)\n",
    "            .agg({\"standard_value\":\"median\",\"canonical_smiles\":\"first\"}))\n",
    "    return df\n",
    "\n",
    "df1 = clean(df1); df1[\"protein\"] = \"COX1\"; df1[\"label\"] = 0\n",
    "df2 = clean(df2); df2[\"protein\"] = \"COX2\"; df2[\"label\"] = 1\n",
    "pairs = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# 6) SMILES faltantes\n",
    "def fetch_smiles(mol_id):\n",
    "    url = f\"https://www.ebi.ac.uk/chembl/api/data/molecule/{mol_id}.json\"\n",
    "    try:\n",
    "        r = http_get(url, timeout=20)\n",
    "        j = r.json().get(\"molecule_structures\", {})\n",
    "        return j.get(\"canonical_smiles\", None)\n",
    "    except requests.exceptions.RequestException:\n",
    "        return None\n",
    "\n",
    "mask_missing = pairs[\"canonical_smiles\"].isna() | (pairs[\"canonical_smiles\"]==\"\")\n",
    "need = pairs.loc[mask_missing, \"molecule_chembl_id\"].dropna().unique().tolist()\n",
    "smi_cache = {}\n",
    "for m in need:\n",
    "    smi_cache[m] = fetch_smiles(m)\n",
    "    time.sleep(0.05)\n",
    "pairs.loc[mask_missing, \"canonical_smiles\"] = pairs.loc[mask_missing, \"molecule_chembl_id\"].map(smi_cache)\n",
    "pairs = pairs.dropna(subset=[\"canonical_smiles\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"pairs shape:\", pairs.shape)\n",
    "\n",
    "# 7) Huellas simples por SMILES (3-gramas con hashing) - sin RDKit\n",
    "def smiles_to_ngram(smi, nbits=1024):\n",
    "    s = re.sub(r\"\\s+\", \"\", smi or \"\")\n",
    "    grams = [s[i:i+3] for i in range(max(1, len(s)-2))]\n",
    "    v = np.zeros(nbits, dtype=np.int8)\n",
    "    for g in grams:\n",
    "        v[hash(g) % nbits] = 1\n",
    "    return v\n",
    "\n",
    "uniq = pairs[[\"molecule_chembl_id\",\"canonical_smiles\"]].drop_duplicates().reset_index(drop=True)\n",
    "fp_mat = np.vstack([smiles_to_ngram(s) for s in uniq[\"canonical_smiles\"]])\n",
    "fp_df = pd.DataFrame(fp_mat, columns=[f\"fp_{i}\" for i in range(fp_mat.shape[1])])\n",
    "fp_df.insert(0, \"molecule_chembl_id\", uniq[\"molecule_chembl_id\"].values)\n",
    "\n",
    "# 8) Rasgos de proteína\n",
    "if PROT_FEATURES == \"aa\":\n",
    "    prot_df = pd.DataFrame({\n",
    "        \"protein\":[\"COX1\",\"COX2\"],\n",
    "        **{f\"aa_{aa}\":[aa_cox1[i], aa_cox2[i]] for i, aa in enumerate(AA)}\n",
    "    })\n",
    "else:\n",
    "    # Fallback one hot si no hubo FASTA\n",
    "    prot_df = pd.DataFrame({\n",
    "        \"protein\":[\"COX1\",\"COX2\"],\n",
    "        \"prot_COX1\":[1,0],\n",
    "        \"prot_COX2\":[0,1],\n",
    "    })\n",
    "\n",
    "# 9) Tabla final y modelo\n",
    "dfm = pairs.merge(fp_df, on=\"molecule_chembl_id\", how=\"left\").merge(prot_df, on=\"protein\", how=\"left\")\n",
    "feat_cols = [c for c in dfm.columns if c.startswith(\"fp_\")]\n",
    "if PROT_FEATURES == \"aa\":\n",
    "    feat_cols += [c for c in dfm.columns if c.startswith(\"aa_\")]\n",
    "else:\n",
    "    feat_cols += [\"prot_COX1\",\"prot_COX2\"]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "X = dfm[feat_cols].values\n",
    "y = dfm[\"label\"].values\n",
    "Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=200, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "clf.fit(Xtr, ytr)\n",
    "yp = clf.predict(Xte)\n",
    "ypb = clf.predict_proba(Xte)[:,1]\n",
    "\n",
    "print(\"dfm shape:\", dfm.shape)\n",
    "print(\"protein features:\", PROT_FEATURES)\n",
    "print(\"accuracy:\", round(accuracy_score(yte, yp), 3))\n",
    "print(\"precision:\", round(precision_score(yte, yp), 3))\n",
    "print(\"recall:\", round(recall_score(yte, yp), 3))\n",
    "print(\"f1:\", round(f1_score(yte, yp), 3))\n",
    "print(\"roc_auc:\", round(roc_auc_score(yte, ypb), 3))\n",
    "\n",
    "# 10) Guardados mínimos\n",
    "pairs.to_csv(\"chembl_pairs_ic50.csv\", index=False)\n",
    "dfm.to_csv(\"pcm_dataset.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84310797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading activities...\n",
      "raw records: 639 191\n",
      "total pairs: 580\n",
      "unique molecules: 580\n",
      "by class: {'COX1': 449, 'COX2': 131}\n",
      "  molecule_chembl_id target_chembl_id  standard_value  \\\n",
      "0       CHEMBL102714    CHEMBL2095188            54.5   \n",
      "1      CHEMBL1076156    CHEMBL2095188         15000.0   \n",
      "2      CHEMBL1081243    CHEMBL2095188          1400.0   \n",
      "3      CHEMBL1081244    CHEMBL2095188          7000.0   \n",
      "4      CHEMBL1081557    CHEMBL2095188         38000.0   \n",
      "\n",
      "                                    canonical_smiles protein  label  \n",
      "0     Cn1cc(C2=C(c3ccc(Cl)cc3Cl)C(=O)NC2=O)c2ccccc21    COX1      0  \n",
      "1  CN1C(=O)/C(=C/c2ccc3c(c2)OCO3)N=C1NCC/N=C/c1cc...    COX1      0  \n",
      "2          CN1C(=O)/C(=C/c2ccc3c(c2)OCO3)N=C1NCCN.Cl    COX1      0  \n",
      "3  CN1C(=O)/C(=C/c2ccc3c(c2)OCO3)N=C1NCC/N=C/c1cc...    COX1      0  \n",
      "4  CN1C(=O)/C(=C/c2ccc3c(c2)OCO3)N=C1NCC/N=C/c1cc...    COX1      0  \n",
      "done: step1_pairs_ic50.csv\n"
     ]
    }
   ],
   "source": [
    "# FINAL CODE #1\n",
    "\n",
    "# Step 1: Download and clean activities from ChEMBL for COX1 and COX2\n",
    "# Required: requests, pandas\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# Session with retries\n",
    "from requests.adapters import HTTPAdapter\n",
    "try:\n",
    "    from urllib3.util.retry import Retry\n",
    "except Exception:\n",
    "    from urllib3.util import Retry  # compatibility\n",
    "\n",
    "SESSION = requests.Session()\n",
    "SESSION.headers.update({\"User-Agent\": \"Sandra-Capstone/1.0\", \"Accept\": \"application/json\"})\n",
    "retries = Retry(\n",
    "    total=5, connect=5, read=5, backoff_factor=0.6,\n",
    "    status_forcelist=[429, 500, 502, 503, 504],\n",
    "    allowed_methods=frozenset([\"GET\"])\n",
    ")\n",
    "adapter = HTTPAdapter(max_retries=retries)\n",
    "SESSION.mount(\"https://\", adapter)\n",
    "SESSION.mount(\"http://\", adapter)\n",
    "\n",
    "def http_get_json(url, params=None, timeout=25):\n",
    "    r = SESSION.get(url, params=params, timeout=timeout)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "# Project parameters\n",
    "CHEMBL_COX1 = \"CHEMBL2095188\"\n",
    "CHEMBL_COX2 = \"CHEMBL2094253\"\n",
    "STD_TYPE = \"IC50\"\n",
    "STD_UNITS = \"nM\"\n",
    "\n",
    "# Paginated download of activities\n",
    "def get_activities(target_id, max_pages=20, page_size=200):\n",
    "    base = \"https://www.ebi.ac.uk/chembl/api/data/activity.json\"\n",
    "    rows = []\n",
    "    for p in range(max_pages):\n",
    "        params = {\n",
    "            \"target_chembl_id\": target_id,\n",
    "            \"standard_type\": STD_TYPE,\n",
    "            \"standard_units\": STD_UNITS,\n",
    "            \"limit\": page_size,\n",
    "            \"offset\": p * page_size\n",
    "        }\n",
    "        try:\n",
    "            data = http_get_json(base, params=params)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"notice: failed page {p} for {target_id}: {e}\")\n",
    "            break\n",
    "        items = data.get(\"activities\", [])\n",
    "        if not items:\n",
    "            break\n",
    "        rows.extend(items)\n",
    "        time.sleep(0.1)\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "print(\"downloading activities...\")\n",
    "df1 = get_activities(CHEMBL_COX1)\n",
    "df2 = get_activities(CHEMBL_COX2)\n",
    "print(\"raw records:\", len(df1), len(df2))\n",
    "\n",
    "# Basic cleaning and labeling\n",
    "for df in (df1, df2):\n",
    "    df[\"standard_value\"] = pd.to_numeric(df[\"standard_value\"], errors=\"coerce\")\n",
    "\n",
    "def clean(df):\n",
    "    cols = [\"molecule_chembl_id\", \"target_chembl_id\", \"standard_value\", \"standard_relation\", \"canonical_smiles\"]\n",
    "    for c in cols:\n",
    "        if c not in df.columns:\n",
    "            df[c] = None\n",
    "    # For now we only keep relation equal to avoid inequalities\n",
    "    df = df[df[\"standard_relation\"].fillna(\"=\") == \"=\"]\n",
    "    df = df.dropna(subset=[\"standard_value\"])\n",
    "    df = df[df[\"standard_value\"] > 0]\n",
    "    # Collapse duplicates by molecule and target using median IC50\n",
    "    df = (\n",
    "        df.groupby([\"molecule_chembl_id\", \"target_chembl_id\"], as_index=False)\n",
    "          .agg({\"standard_value\": \"median\", \"canonical_smiles\": \"first\"})\n",
    "    )\n",
    "    return df\n",
    "\n",
    "df1 = clean(df1); df1[\"protein\"] = \"COX1\"; df1[\"label\"] = 0\n",
    "df2 = clean(df2); df2[\"protein\"] = \"COX2\"; df2[\"label\"] = 1\n",
    "pairs = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# Ensure SMILES if missing\n",
    "def fetch_smiles(mol_id):\n",
    "    url = f\"https://www.ebi.ac.uk/chembl/api/data/molecule/{mol_id}.json\"\n",
    "    try:\n",
    "        j = http_get_json(url)\n",
    "    except requests.exceptions.RequestException:\n",
    "        return None\n",
    "    return j.get(\"molecule_structures\", {}).get(\"canonical_smiles\", None)\n",
    "\n",
    "mask_missing = pairs[\"canonical_smiles\"].isna() | (pairs[\"canonical_smiles\"] == \"\")\n",
    "need = pairs.loc[mask_missing, \"molecule_chembl_id\"].dropna().unique().tolist()\n",
    "cache = {}\n",
    "for m in need:\n",
    "    cache[m] = fetch_smiles(m)\n",
    "    time.sleep(0.05)\n",
    "pairs.loc[mask_missing, \"canonical_smiles\"] = pairs.loc[mask_missing, \"molecule_chembl_id\"].map(cache)\n",
    "\n",
    "# Discard entries without SMILES\n",
    "pairs = pairs.dropna(subset=[\"canonical_smiles\"]).reset_index(drop=True)\n",
    "\n",
    "# Small control views\n",
    "print(\"total pairs:\", len(pairs))\n",
    "print(\"unique molecules:\", pairs[\"molecule_chembl_id\"].nunique())\n",
    "print(\"by class:\", pairs[\"protein\"].value_counts().to_dict())\n",
    "print(pairs.head(5))\n",
    "\n",
    "# Save for the next step\n",
    "pairs.to_csv(\"step1_pairs_ic50.csv\", index=False)\n",
    "print(\"done: step1_pairs_ic50.csv\")\n",
    "\n",
    "### Step 1 download IC50 activity records for COX1 and COX2, clean them, and save molecule target pairs with SMILES. \n",
    "### We downloaded IC50 activity data from ChEMBL for COX1 and COX2, cleaned it, and kept only valid values where the relation was “=”. \n",
    "# We removed duplicates by taking the median IC50 for each molecule–target pair, added SMILES if they were missing, and labeled COX1 as 0 and COX2 as 1. \n",
    "# The results show 580 total pairs (449 COX1 and 131 COX2). This means most of the data is for COX1, and each molecule is linked to only one target in this dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3dd4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Protein feature type: amino_acid_composition\n",
      "  protein      aa_A      aa_C      aa_D      aa_E      aa_F      aa_G  \\\n",
      "0    COX1  0.041736  0.021703  0.040067  0.060100  0.065109  0.075125   \n",
      "1    COX2  0.051325  0.021523  0.043046  0.059603  0.062914  0.061258   \n",
      "\n",
      "       aa_H      aa_I      aa_K  ...      aa_M      aa_N      aa_P      aa_Q  \\\n",
      "0  0.030050  0.041736  0.041736  ...  0.030050  0.031720  0.076795  0.045075   \n",
      "1  0.031457  0.056291  0.056291  ...  0.024834  0.048013  0.066225  0.051325   \n",
      "\n",
      "       aa_R      aa_S      aa_T      aa_V      aa_W      aa_Y  \n",
      "0  0.055092  0.055092  0.050083  0.053422  0.016694  0.045075  \n",
      "1  0.044702  0.057947  0.056291  0.057947  0.009934  0.044702  \n",
      "\n",
      "[2 rows x 21 columns]\n",
      "Saved: step2_protein_features.csv\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Protein features for COX1 and COX2\n",
    "# Requirements: requests, pandas, numpy\n",
    "\n",
    "import requests, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from requests.adapters import HTTPAdapter\n",
    "try:\n",
    "    from urllib3.util.retry import Retry\n",
    "except Exception:\n",
    "    from urllib3.util import Retry  # compatibility\n",
    "\n",
    "# Reuse a robust session\n",
    "SESSION = requests.Session()\n",
    "SESSION.headers.update({\"User-Agent\": \"Sandra-Capstone/1.0\"})\n",
    "retries = Retry(\n",
    "    total=5, connect=5, read=5, backoff_factor=0.6,\n",
    "    status_forcelist=[429, 500, 502, 503, 504],\n",
    "    allowed_methods=frozenset([\"GET\"])\n",
    ")\n",
    "adapter = HTTPAdapter(max_retries=retries)\n",
    "SESSION.mount(\"https://\", adapter)\n",
    "SESSION.mount(\"http://\", adapter)\n",
    "\n",
    "VERIFY = True\n",
    "try:\n",
    "    import certifi\n",
    "    VERIFY = certifi.where()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# UniProt IDs\n",
    "UNIPROT_COX1 = \"P23219\"\n",
    "UNIPROT_COX2 = \"P35354\"\n",
    "\n",
    "# Try multiple UniProt endpoints for reliability\n",
    "def get_fasta_seq(uniprot_id, timeout=15):\n",
    "    urls = [\n",
    "        f\"https://rest.uniprot.org/uniprotkb/{uniprot_id}.fasta\",\n",
    "        f\"https://www.uniprot.org/uniprotkb/{uniprot_id}.fasta\",\n",
    "        f\"https://legacy.uniprot.org/uniprotkb/{uniprot_id}.fasta\",\n",
    "    ]\n",
    "    last_err = None\n",
    "    for url in urls:\n",
    "        try:\n",
    "            r = SESSION.get(url, timeout=timeout, verify=VERIFY)\n",
    "            if r.status_code == 200 and \">\" in r.text:\n",
    "                return \"\".join([ln.strip() for ln in r.text.splitlines() if not ln.startswith(\">\")])\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            last_err = e\n",
    "            time.sleep(0.8)\n",
    "    print(f\"Warning: Could not get FASTA for {uniprot_id}. Last error: {last_err}\")\n",
    "    return None\n",
    "\n",
    "# Get sequences\n",
    "seq_cox1 = get_fasta_seq(UNIPROT_COX1)\n",
    "seq_cox2 = get_fasta_seq(UNIPROT_COX2)\n",
    "\n",
    "# Amino acid composition\n",
    "AA = list(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "def aa_comp(seq):\n",
    "    n = len(seq)\n",
    "    return np.array([seq.count(a) / n for a in AA], dtype=float)\n",
    "\n",
    "# Build dataframe\n",
    "if seq_cox1 and seq_cox2:\n",
    "    aa_cox1 = aa_comp(seq_cox1)\n",
    "    aa_cox2 = aa_comp(seq_cox2)\n",
    "    prot_df = pd.DataFrame({\n",
    "        \"protein\": [\"COX1\", \"COX2\"],\n",
    "        **{f\"aa_{aa}\": [aa_cox1[i], aa_cox2[i]] for i, aa in enumerate(AA)}\n",
    "    })\n",
    "    feature_type = \"amino_acid_composition\"\n",
    "else:\n",
    "    prot_df = pd.DataFrame({\n",
    "        \"protein\": [\"COX1\", \"COX2\"],\n",
    "        \"prot_COX1\": [1, 0],\n",
    "        \"prot_COX2\": [0, 1]\n",
    "    })\n",
    "    feature_type = \"one_hot\"\n",
    "\n",
    "print(f\"Protein feature type: {feature_type}\")\n",
    "print(prot_df)\n",
    "\n",
    "# Save\n",
    "prot_df.to_csv(\"step2_protein_features.csv\", index=False)\n",
    "print(\"Saved: step2_protein_features.csv\")\n",
    "\n",
    "### Get the amino acid sequences for COX1 and COX2 from UniProt and turn each sequence into a 20 feature vector of amino acid composition.\n",
    "### We got the amino acid sequences for COX1 and COX2 from UniProt and calculated the fraction of each of the 20 amino acids in the sequence. \n",
    "# This gives a simple numeric profile for each protein. \n",
    "# The results show, for example, that COX1 has 7.51% glycine, and COX2 has 6.13% glycine. These values will be used later as protein features in the PCM model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be83d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pairs: (580, 6)\n",
      "Unique molecules: (580, 2)\n",
      "Saved: step3_ligand_features.csv\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Ligand features from SMILES (temporary n-gram method)\n",
    "# Requirements: pandas, numpy, re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Load cleaned pairs from Step 1\n",
    "pairs = pd.read_csv(\"step1_pairs_ic50.csv\")\n",
    "print(\"Loaded pairs:\", pairs.shape)\n",
    "\n",
    "# Function: convert SMILES to hashed n-gram vector\n",
    "def smiles_to_ngram(smi, nbits=1024):\n",
    "    s = re.sub(r\"\\s+\", \"\", smi or \"\")\n",
    "    grams = [s[i:i+3] for i in range(max(1, len(s)-2))]\n",
    "    v = np.zeros(nbits, dtype=np.int8)\n",
    "    for g in grams:\n",
    "        v[hash(g) % nbits] = 1\n",
    "    return v\n",
    "\n",
    "# Get unique molecules\n",
    "uniq = pairs[[\"molecule_chembl_id\", \"canonical_smiles\"]].drop_duplicates().reset_index(drop=True)\n",
    "print(\"Unique molecules:\", uniq.shape)\n",
    "\n",
    "# Build feature matrix\n",
    "fp_mat = np.vstack([smiles_to_ngram(s) for s in uniq[\"canonical_smiles\"]])\n",
    "fp_df = pd.DataFrame(fp_mat, columns=[f\"fp_{i}\" for i in range(fp_mat.shape[1])])\n",
    "fp_df.insert(0, \"molecule_chembl_id\", uniq[\"molecule_chembl_id\"].values)\n",
    "\n",
    "# Save features\n",
    "fp_df.to_csv(\"step3_ligand_features.csv\", index=False)\n",
    "print(\"Saved: step3_ligand_features.csv\")\n",
    "\n",
    "### turn each SMILES into a 1024 bit hashed ngram fingerprint without RDKit.\n",
    "### Loaded pairs: (580, 6) and Unique molecules: (580, 2) shows one molecule per pair in this cleaned set.\n",
    "\n",
    "### We converted each molecule’s SMILES string into a 1024-bit vector using hashed 3-character n-grams (a simple way to represent molecular structure). \n",
    "# This creates ligand fingerprints without needing RDKit. The results show 580 unique molecules, each with its own fingerprint, saved for later steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f826b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK: 2.3.2 1.16.1 1.7.1\n"
     ]
    }
   ],
   "source": [
    "import numpy, scipy, sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "print(\"OK:\", numpy.__version__, scipy.__version__, sklearn.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eeb48717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairs: (580, 6)\n",
      "Protein features: (2, 21)\n",
      "Ligand features: (580, 1025)\n",
      "Merged dataset: (580, 1050)\n",
      "Metrics: {'accuracy': 1.0, 'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'roc_auc': 1.0}\n",
      "Saved: step4_merged_dataset.csv, step4_model_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "# NOT CORRECT, DATA TOO PERFECT\n",
    "# Step 4: Merge datasets and train model\n",
    "# Requirements: pandas, numpy, scikit-learn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Load all datasets\n",
    "pairs = pd.read_csv(\"step1_pairs_ic50.csv\")\n",
    "prot_df = pd.read_csv(\"step2_protein_features.csv\")\n",
    "lig_df = pd.read_csv(\"step3_ligand_features.csv\")\n",
    "\n",
    "print(\"Pairs:\", pairs.shape)\n",
    "print(\"Protein features:\", prot_df.shape)\n",
    "print(\"Ligand features:\", lig_df.shape)\n",
    "\n",
    "# Merge ligand features\n",
    "merged = pairs.merge(lig_df, on=\"molecule_chembl_id\", how=\"left\")\n",
    "# Merge protein features\n",
    "merged = merged.merge(prot_df, on=\"protein\", how=\"left\")\n",
    "\n",
    "print(\"Merged dataset:\", merged.shape)\n",
    "\n",
    "# Define features\n",
    "feature_cols = [c for c in merged.columns if c.startswith(\"fp_\") or c.startswith(\"aa_\") or c.startswith(\"prot_\")]\n",
    "X = merged[feature_cols].values\n",
    "y = merged[\"label\"].values\n",
    "\n",
    "# Split train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Train Random Forest\n",
    "clf = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "y_prob = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "metrics = {\n",
    "    \"accuracy\": round(accuracy_score(y_test, y_pred), 3),\n",
    "    \"precision\": round(precision_score(y_test, y_pred), 3),\n",
    "    \"recall\": round(recall_score(y_test, y_pred), 3),\n",
    "    \"f1\": round(f1_score(y_test, y_pred), 3),\n",
    "    \"roc_auc\": round(roc_auc_score(y_test, y_prob), 3)\n",
    "}\n",
    "print(\"Metrics:\", metrics)\n",
    "\n",
    "# Save merged dataset and metrics\n",
    "merged.to_csv(\"step4_merged_dataset.csv\", index=False)\n",
    "pd.DataFrame([metrics]).to_csv(\"step4_model_metrics.csv\", index=False)\n",
    "print(\"Saved: step4_merged_dataset.csv, step4_model_metrics.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d78ad0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COX2 molecules (with pChEMBL): 87\n",
      "Class balance: {0: 48, 1: 39}\n",
      "COX2 activity metrics (group split by molecule): {'accuracy': 0.722, 'precision': 0.833, 'recall': 0.556, 'f1': 0.667, 'roc_auc': 0.71, 'n_train': 69, 'n_test': 18, 'confusion_matrix': {'tn': 8, 'fp': 1, 'fn': 4, 'tp': 5}, 'threshold_pchembl_active': 6.0}\n",
      "Saved: step4d_cox2_activity_dataset.csv, step4d_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Single-target activity classification for COX2 using pChEMBL\n",
    "# Requirements: pandas, numpy, scikit-learn, re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "# ### 1) Load pChEMBL pairs and keep only COX2\n",
    "pairs_p = pd.read_csv(\"step1b_pairs_pchembl.csv\")   # columns: molecule_chembl_id, protein, pchembl_value, canonical_smiles\n",
    "cox2 = pairs_p[pairs_p[\"protein\"] == \"COX2\"].copy()\n",
    "\n",
    "# Keep one pChEMBL per molecule (max potency)\n",
    "cox2 = (cox2.groupby([\"molecule_chembl_id\"], as_index=False)\n",
    "             .agg({\"pchembl_value\": \"max\", \"canonical_smiles\": \"first\"}))\n",
    "\n",
    "print(\"COX2 molecules (with pChEMBL):\", cox2.shape[0])\n",
    "\n",
    "# ### 2) Define activity labels: Active >= 6.0 (≈ ≤ 1 μM), else Inactive\n",
    "THRESH = 6.0\n",
    "cox2[\"label_active\"] = (cox2[\"pchembl_value\"] >= THRESH).astype(int)\n",
    "print(\"Class balance:\", cox2[\"label_active\"].value_counts().to_dict())\n",
    "\n",
    "# ### 3) Create ligand features (hashed SMILES n-grams; RDKit-free)\n",
    "def smiles_to_ngram(smi, nbits=1024):\n",
    "    s = re.sub(r\"\\s+\", \"\", smi or \"\")\n",
    "    grams = [s[i:i+3] for i in range(max(1, len(s)-2))]\n",
    "    v = np.zeros(nbits, dtype=np.int8)\n",
    "    for g in grams:\n",
    "        v[hash(g) % nbits] = 1\n",
    "    return v\n",
    "\n",
    "uniq = cox2[[\"molecule_chembl_id\", \"canonical_smiles\"]].drop_duplicates().reset_index(drop=True)\n",
    "fp_mat = np.vstack([smiles_to_ngram(s) for s in uniq[\"canonical_smiles\"]])\n",
    "fp_cols = [f\"fp_{i}\" for i in range(fp_mat.shape[1])]\n",
    "fp_df = pd.DataFrame(fp_mat, columns=fp_cols)\n",
    "fp_df.insert(0, \"molecule_chembl_id\", uniq[\"molecule_chembl_id\"].values)\n",
    "\n",
    "# Merge fingerprints into COX2 table\n",
    "data = cox2.merge(fp_df, on=\"molecule_chembl_id\", how=\"inner\")\n",
    "\n",
    "# ### 4) Build X, y and group split by molecule\n",
    "feature_cols = [c for c in data.columns if c.startswith(\"fp_\")]\n",
    "X = data[feature_cols].values\n",
    "y = data[\"label_active\"].values\n",
    "groups = data[\"molecule_chembl_id\"].values\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "tr_idx, te_idx = next(gss.split(X, y, groups))\n",
    "Xtr, Xte = X[tr_idx], X[te_idx]\n",
    "ytr, yte = y[tr_idx], y[te_idx]\n",
    "\n",
    "# ### 5) Train a simple Random Forest\n",
    "clf = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "clf.fit(Xtr, ytr)\n",
    "yp = clf.predict(Xte)\n",
    "ypb = clf.predict_proba(Xte)[:, 1]\n",
    "\n",
    "# ### 6) Metrics + confusion matrix\n",
    "tn, fp, fn, tp = confusion_matrix(yte, yp, labels=[0,1]).ravel()\n",
    "metrics = {\n",
    "    \"accuracy\": round(accuracy_score(yte, yp), 3),\n",
    "    \"precision\": round(precision_score(yte, yp), 3),\n",
    "    \"recall\": round(recall_score(yte, yp), 3),\n",
    "    \"f1\": round(f1_score(yte, yp), 3),\n",
    "    \"roc_auc\": round(roc_auc_score(yte, ypb), 3) if len(np.unique(yte)) > 1 else float(\"nan\"),\n",
    "    \"n_train\": int(len(ytr)),\n",
    "    \"n_test\": int(len(yte)),\n",
    "    \"confusion_matrix\": {\"tn\": int(tn), \"fp\": int(fp), \"fn\": int(fn), \"tp\": int(tp)},\n",
    "    \"threshold_pchembl_active\": THRESH\n",
    "}\n",
    "print(\"COX2 activity metrics (group split by molecule):\", metrics)\n",
    "\n",
    "# ### 7) Save final dataset and metrics\n",
    "out = data.copy()\n",
    "out[\"split\"] = \"train\"\n",
    "out.loc[te_idx, \"split\"] = \"test\"\n",
    "out.to_csv(\"step4d_cox2_activity_dataset.csv\", index=False)\n",
    "pd.DataFrame([metrics]).to_csv(\"step4d_metrics.csv\", index=False)\n",
    "print(\"Saved: step4d_cox2_activity_dataset.csv, step4d_metrics.csv\")\n",
    "\n",
    "### This is a single target COX2 classifier that predicts active vs inactive from ligand features only\n",
    "### We built a COX2-only classification model using pChEMBL values to label molecules as active (≥6.0) or inactive. \n",
    "# We split the data so the same molecule could not be in both train and test sets. The results show the model is 72% accurate, with high precision (83%) but lower recall (56%), meaning it’s good at avoiding false positives but misses many actives.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f39497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 features by importance:\n",
      "fp_530: 0.0345\n",
      "fp_36: 0.0321\n",
      "fp_204: 0.0291\n",
      "fp_599: 0.0239\n",
      "fp_533: 0.0220\n",
      "fp_797: 0.0219\n",
      "fp_627: 0.0201\n",
      "fp_275: 0.0171\n",
      "fp_517: 0.0162\n",
      "fp_846: 0.0154\n",
      "fp_427: 0.0152\n",
      "fp_127: 0.0149\n",
      "fp_544: 0.0138\n",
      "fp_869: 0.0134\n",
      "fp_91: 0.0129\n",
      "fp_1022: 0.0126\n",
      "fp_299: 0.0124\n",
      "fp_586: 0.0123\n",
      "fp_463: 0.0121\n",
      "fp_865: 0.0121\n",
      "Saved: step5_feature_importances.csv\n",
      "Saved: step5_cumulative_importance.png\n",
      "Saved: step5_pca_cox2.png\n",
      "PCA explained variance ratio: PC1=0.175, PC2=0.118\n"
     ]
    }
   ],
   "source": [
    "# STEP 5: Feature Importance and Interpretability for COX2 Model (robust, headless-safe)\n",
    "# Requirements: pandas, numpy, matplotlib, scikit-learn\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Use a non-interactive backend to avoid GUI issues\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# ### 1) Load dataset\n",
    "in_path = \"step4d_cox2_activity_dataset.csv\"\n",
    "if not os.path.exists(in_path):\n",
    "    raise FileNotFoundError(f\"Expected file '{in_path}' not found. Run Step 4d first.\")\n",
    "\n",
    "data = pd.read_csv(in_path)\n",
    "\n",
    "# ### 2) Select features and labels\n",
    "feature_cols = [c for c in data.columns if c.startswith(\"fp_\")]\n",
    "if not feature_cols:\n",
    "    raise ValueError(\"No fingerprint features (fp_*) found. Check Step 3 output and merging.\")\n",
    "\n",
    "X = data[feature_cols].to_numpy(dtype=np.float32)\n",
    "y = data[\"label_active\"].to_numpy(dtype=np.int32)\n",
    "\n",
    "# ### 3) Train Random Forest (fit on all data just to inspect importances)\n",
    "clf = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# ### 4) Feature importances and top-20 ranking\n",
    "importances = clf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "top_n = 20\n",
    "top_features = [(feature_cols[i], float(importances[i])) for i in indices[:top_n]]\n",
    "print(\"Top 20 features by importance:\")\n",
    "for feat, val in top_features:\n",
    "    print(f\"{feat}: {val:.4f}\")\n",
    "\n",
    "# Save full importance table\n",
    "feat_import_df = pd.DataFrame({\n",
    "    \"feature\": [feature_cols[i] for i in indices],\n",
    "    \"importance\": importances[indices].astype(float)\n",
    "})\n",
    "feat_import_df.to_csv(\"step5_feature_importances.csv\", index=False)\n",
    "print(\"Saved: step5_feature_importances.csv\")\n",
    "\n",
    "# ### 5) Plot cumulative importance\n",
    "sorted_importances = importances[indices]\n",
    "cumulative_importance = np.cumsum(sorted_importances)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(range(1, len(cumulative_importance)+1), cumulative_importance, marker=\"o\")\n",
    "plt.xlabel(\"Number of Features\")\n",
    "plt.ylabel(\"Cumulative Importance\")\n",
    "plt.title(\"Cumulative Feature Importance (RF - COX2)\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"step5_cumulative_importance.png\", dpi=150)\n",
    "plt.close()\n",
    "print(\"Saved: step5_cumulative_importance.png\")\n",
    "\n",
    "# ### 6) PCA visualization (2D projection)\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "labels = np.unique(y)\n",
    "for label in labels:\n",
    "    mask = (y == label)\n",
    "    plt.scatter(X_pca[mask, 0], X_pca[mask, 1], label=f\"Active={int(label)}\", alpha=0.7)\n",
    "plt.xlabel(\"PCA 1\")\n",
    "plt.ylabel(\"PCA 2\")\n",
    "plt.title(\"PCA of Fingerprint Features - COX2\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"step5_pca_cox2.png\", dpi=150)\n",
    "plt.close()\n",
    "print(\"Saved: step5_pca_cox2.png\")\n",
    "\n",
    "# ### 7) Print PCA explained variance\n",
    "expl_var = pca.explained_variance_ratio_\n",
    "print(f\"PCA explained variance ratio: PC1={expl_var[0]:.3f}, PC2={expl_var[1]:.3f}\")\n",
    "\n",
    "### 5) Inspect feature importance and a simple PCA view.\n",
    "### We looked at which fingerprint bits were most important for the COX2 model and did a PCA plot to see how the data looks in two dimensions. \n",
    "# The top features have importances between 0.0345 and 0.0121. \n",
    "# The PCA shows that the first two components explain about 29% of the variance, so there’s only partial visual separation between active and inactive molecules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd556038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCM Model metrics (group split): {'accuracy': 0.85, 'precision': 0.873, 'recall': 0.899, 'f1': 0.886, 'roc_auc': 0.879, 'n_train': 428, 'n_test': 107, 'confusion_matrix': {'tn': 29, 'fp': 9, 'fn': 7, 'tp': 62}, 'threshold_pchembl_active': 6.0}\n",
      "Saved: step6_pcm_dataset.csv, step6_pcm_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "# STEP 6: Proteochemometric (PCM) Model for COX1 and COX2\n",
    "# Goal: Combine ligand fingerprints + protein amino acid composition features\n",
    "#       to train a joint model predicting activity across both targets.\n",
    "# Requirements: pandas, numpy, scikit-learn\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "# --- 1) Load ligand features (Step 3) ---\n",
    "lig_path = \"step3_ligand_features.csv\"\n",
    "if not os.path.exists(lig_path):\n",
    "    raise FileNotFoundError(f\"Expected ligand file '{lig_path}' not found. Run Step 3 first.\")\n",
    "lig_feats = pd.read_csv(lig_path)  # contains: molecule_chembl_id + fp_* features\n",
    "\n",
    "# --- 2) Load protein features (Step 2) ---\n",
    "prot_path = \"step2_protein_features.csv\"\n",
    "if not os.path.exists(prot_path):\n",
    "    raise FileNotFoundError(f\"Expected protein file '{prot_path}' not found. Run Step 2 first.\")\n",
    "prot_feats = pd.read_csv(prot_path)  # contains: protein + AA composition features\n",
    "\n",
    "# --- 3) Load pairs table with pChEMBL values (Step 1) ---\n",
    "pairs_path = \"step1b_pairs_pchembl.csv\"\n",
    "if not os.path.exists(pairs_path):\n",
    "    raise FileNotFoundError(f\"Expected pairs file '{pairs_path}' not found. Run Step 1 first.\")\n",
    "pairs = pd.read_csv(pairs_path)\n",
    "\n",
    "# --- 4) Define activity labels: Active >= 6.0, else Inactive ---\n",
    "THRESH = 6.0\n",
    "pairs[\"label_active\"] = (pairs[\"pchembl_value\"] >= THRESH).astype(int)\n",
    "\n",
    "# --- 5) Merge ligand + protein features into PCM table ---\n",
    "df = pairs.merge(lig_feats, on=\"molecule_chembl_id\", how=\"inner\")\n",
    "df = df.merge(prot_feats, on=\"protein\", how=\"inner\")\n",
    "\n",
    "# --- 6) Feature matrix and labels ---\n",
    "feature_cols = [c for c in df.columns if c.startswith(\"fp_\") or len(c) == 1]  # fp_* and AA letters\n",
    "X = df[feature_cols].to_numpy(dtype=np.float32)\n",
    "y = df[\"label_active\"].to_numpy(dtype=np.int32)\n",
    "\n",
    "# Groups: molecule+protein to prevent leakage\n",
    "groups = df[\"molecule_chembl_id\"].astype(str) + \"_\" + df[\"protein\"].astype(str)\n",
    "\n",
    "# --- 7) Group split (80% train / 20% test) ---\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, test_idx = next(gss.split(X, y, groups))\n",
    "\n",
    "X_train, X_test = X[train_idx], X[test_idx]\n",
    "y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "# --- 8) Train Random Forest ---\n",
    "clf = RandomForestClassifier(n_estimators=300, random_state=42, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# --- 9) Evaluate ---\n",
    "y_pred = clf.predict(X_test)\n",
    "y_prob = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred, labels=[0, 1]).ravel()\n",
    "metrics = {\n",
    "    \"accuracy\": round(accuracy_score(y_test, y_pred), 3),\n",
    "    \"precision\": round(precision_score(y_test, y_pred), 3),\n",
    "    \"recall\": round(recall_score(y_test, y_pred), 3),\n",
    "    \"f1\": round(f1_score(y_test, y_pred), 3),\n",
    "    \"roc_auc\": round(roc_auc_score(y_test, y_prob), 3) if len(np.unique(y_test)) > 1 else float(\"nan\"),\n",
    "    \"n_train\": int(len(y_train)),\n",
    "    \"n_test\": int(len(y_test)),\n",
    "    \"confusion_matrix\": {\"tn\": int(tn), \"fp\": int(fp), \"fn\": int(fn), \"tp\": int(tp)},\n",
    "    \"threshold_pchembl_active\": THRESH\n",
    "}\n",
    "\n",
    "print(\"PCM Model metrics (group split):\", metrics)\n",
    "\n",
    "# --- 10) Save dataset and metrics ---\n",
    "df_out = df.copy()\n",
    "df_out[\"split\"] = \"train\"\n",
    "df_out.loc[test_idx, \"split\"] = \"test\"\n",
    "df_out.to_csv(\"step6_pcm_dataset.csv\", index=False)\n",
    "pd.DataFrame([metrics]).to_csv(\"step6_pcm_metrics.csv\", index=False)\n",
    "\n",
    "print(\"Saved: step6_pcm_dataset.csv, step6_pcm_metrics.csv\")\n",
    "\n",
    "### is the PCM model that concatenates ligand fingerprints and protein features to predict activity across both targets.\n",
    "###We trained a proteochemometric (PCM) model combining ligand fingerprints and protein amino acid composition features for both COX1 and COX2. The results show 85% accuracy, high precision (87%), and high recall (90%). \n",
    "# This means the model catches most actives while still keeping false positives relatively low. It performs better than the COX2-only model because it uses both ligand and protein information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3de9cfd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    model                                        best_params  accuracy  \\\n",
      "2      rf  {'max_depth': 30, 'min_samples_leaf': 1, 'n_es...  0.859813   \n",
      "1     knn  {'clf__n_neighbors': 3, 'clf__weights': 'dista...  0.859813   \n",
      "0  logreg                                         {'C': 0.1}  0.813084   \n",
      "\n",
      "   precision    recall        f1   roc_auc  avg_precision  \n",
      "2   0.875000  0.913043  0.893617  0.878909       0.915117  \n",
      "1   0.875000  0.913043  0.893617  0.862891       0.885537  \n",
      "0   0.835616  0.884058  0.859155  0.860984       0.889470  \n",
      "Best model: rf with params: RandomForestClassifier(max_depth=30, n_estimators=400, n_jobs=-1,\n",
      "                       random_state=42)\n",
      "Brier score (lower is better): 0.1254\n",
      "Network figure skipped: No module named 'networkx'\n"
     ]
    }
   ],
   "source": [
    "# STEP 7 — MODEL COMPARISON, SELECTION, AND EXTRA VISUALIZATIONS\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, learning_curve\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve, precision_recall_curve, average_precision_score,\n",
    "    confusion_matrix, classification_report, brier_score_loss\n",
    ")\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "# 1) Define models and parameter grids\n",
    "models_and_params = [\n",
    "    (\"logreg\",\n",
    "     LogisticRegression(max_iter=5000, solver=\"lbfgs\"),\n",
    "     {\"C\": [0.01, 0.1, 1, 10]}\n",
    "    ),\n",
    "    (\"knn\",\n",
    "     Pipeline([(\"scaler\", StandardScaler()), (\"clf\", KNeighborsClassifier())]),\n",
    "     {\"clf__n_neighbors\": [3, 5, 7, 11], \"clf__weights\": [\"uniform\", \"distance\"]}\n",
    "    ),\n",
    "    (\"rf\",\n",
    "     RandomForestClassifier(n_estimators=400, random_state=42, n_jobs=-1),\n",
    "     {\"n_estimators\": [200, 400], \"max_depth\": [None, 10, 30], \"min_samples_leaf\": [1, 3]}\n",
    "    ),\n",
    "]\n",
    "\n",
    "# 2) Grid search with five fold cross validation using ROC AUC\n",
    "results = []\n",
    "best_estimators = {}\n",
    "\n",
    "for name, est, param_grid in models_and_params:\n",
    "    gs = GridSearchCV(\n",
    "        estimator=est,\n",
    "        param_grid=param_grid,\n",
    "        scoring=\"roc_auc\",\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "        refit=True,\n",
    "        verbose=0\n",
    "    )\n",
    "    gs.fit(X_train, y_train)\n",
    "    best_estimators[name] = gs.best_estimator_\n",
    "\n",
    "    # evaluate on test\n",
    "    if hasattr(gs.best_estimator_, \"predict_proba\"):\n",
    "        y_prob = gs.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        # fallback for models without predict_proba\n",
    "        y_prob = np.clip(gs.best_estimator_.decision_function(X_test), -20, 20)\n",
    "        y_prob = (y_prob - y_prob.min()) / (y_prob.max() - y_prob.min() + 1e-12)\n",
    "\n",
    "    y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "    res = {\n",
    "        \"model\": name,\n",
    "        \"best_params\": str(gs.best_params_),\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"precision\": precision_score(y_test, y_pred, zero_division=0),\n",
    "        \"recall\": recall_score(y_test, y_pred, zero_division=0),\n",
    "        \"f1\": f1_score(y_test, y_pred, zero_division=0),\n",
    "        \"roc_auc\": roc_auc_score(y_test, y_prob),\n",
    "        \"avg_precision\": average_precision_score(y_test, y_prob),\n",
    "    }\n",
    "    results.append(res)\n",
    "\n",
    "df_results = pd.DataFrame(results).sort_values(\"roc_auc\", ascending=False)\n",
    "df_results.to_csv(\"step7_model_comparison_metrics.csv\", index=False)\n",
    "print(df_results)\n",
    "\n",
    "# pick the overall best by ROC AUC\n",
    "best_name = df_results.iloc[0][\"model\"]\n",
    "best_model = best_estimators[best_name]\n",
    "print(f\"Best model: {best_name} with params: {best_estimators[best_name]}\")\n",
    "\n",
    "# 3) Bar chart comparing models on ROC AUC and F1\n",
    "plt.figure()\n",
    "x = np.arange(len(df_results))\n",
    "plt.bar(x - 0.2, df_results[\"roc_auc\"].values, width=0.4, label=\"ROC AUC\")\n",
    "plt.bar(x + 0.2, df_results[\"f1\"].values, width=0.4, label=\"F1\")\n",
    "plt.xticks(x, df_results[\"model\"].values)\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Model Comparison\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"step7_model_comparison_bar.png\", dpi=200)\n",
    "plt.close()\n",
    "\n",
    "# 4) Calibration curve and Brier score for best model\n",
    "if hasattr(best_model, \"predict_proba\"):\n",
    "    y_prob_best = best_model.predict_proba(X_test)[:, 1]\n",
    "else:\n",
    "    y_dec = np.clip(best_model.decision_function(X_test), -20, 20)\n",
    "    y_prob_best = (y_dec - y_dec.min()) / (y_dec.max() - y_dec.min() + 1e-12)\n",
    "\n",
    "frac_pos, mean_pred = calibration_curve(y_test, y_prob_best, n_bins=10, strategy=\"uniform\")\n",
    "brier = brier_score_loss(y_test, y_prob_best)\n",
    "print(f\"Brier score (lower is better): {brier:.4f}\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "plt.plot(mean_pred, frac_pos, marker=\"o\")\n",
    "plt.xlabel(\"Mean predicted probability\")\n",
    "plt.ylabel(\"Fraction of positives\")\n",
    "plt.title(\"Calibration Curve\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"step7_calibration.png\", dpi=200)\n",
    "plt.close()\n",
    "\n",
    "# 5) Learning curve for best model\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    best_model, X_train, y_train, cv=5, scoring=\"roc_auc\", n_jobs=-1,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 6), shuffle=True, random_state=42\n",
    ")\n",
    "plt.figure()\n",
    "plt.plot(train_sizes, train_scores.mean(axis=1), marker=\"o\", label=\"train\")\n",
    "plt.plot(train_sizes, val_scores.mean(axis=1), marker=\"o\", label=\"cv\")\n",
    "plt.xlabel(\"Training examples\")\n",
    "plt.ylabel(\"ROC AUC\")\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"step7_learning_curve.png\", dpi=200)\n",
    "plt.close()\n",
    "\n",
    "# 6) Probability histogram for positive and negative classes\n",
    "plt.figure()\n",
    "plt.hist(y_prob_best[y_test==0], bins=20, alpha=0.7, label=\"COX1\")\n",
    "plt.hist(y_prob_best[y_test==1], bins=20, alpha=0.7, label=\"COX2\")\n",
    "plt.xlabel(\"Predicted probability of class 1\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Probability Distributions by Class\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"step7_probability_hist.png\", dpi=200)\n",
    "plt.close()\n",
    "\n",
    "# 7) Optional: simple DTI bipartite network from your pairs table\n",
    "# Uses the pairs DataFrame from step 1 (columns: molecule_chembl_id, protein)\n",
    "try:\n",
    "    import networkx as nx\n",
    "    assert \"pairs\" in globals(), \"pairs DataFrame not found. Load step1_pairs_ic50.csv.\"\n",
    "    # If not loaded:\n",
    "    # pairs = pd.read_csv(\"step1_pairs_ic50.csv\")\n",
    "\n",
    "    # build graph\n",
    "    G = nx.Graph()\n",
    "    drugs = pairs[\"molecule_chembl_id\"].dropna().unique().tolist()\n",
    "    targets = pairs[\"protein\"].dropna().unique().tolist()  # expects [\"COX1\",\"COX2\"]\n",
    "\n",
    "    G.add_nodes_from(drugs, bipartite=0)\n",
    "    G.add_nodes_from(targets, bipartite=1)\n",
    "\n",
    "    edges = list(pairs[[\"molecule_chembl_id\", \"protein\"]].itertuples(index=False, name=None))\n",
    "    G.add_edges_from(edges)\n",
    "\n",
    "    deg = dict(G.degree())\n",
    "    deg_df = pd.DataFrame({\"node\": list(deg.keys()), \"degree\": list(deg.values())})\n",
    "    deg_df.to_csv(\"step7_network_degrees.csv\", index=False)\n",
    "\n",
    "    # draw a simple layout\n",
    "    pos = nx.spring_layout(G, seed=42, k=0.3)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=targets, node_size=600)\n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=drugs, node_size=60)\n",
    "    nx.draw_networkx_edges(G, pos, width=0.5)\n",
    "    nx.draw_networkx_labels(G, pos, labels={t: t for t in targets}, font_size=10)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"DTI Bipartite Network\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"step7_dti_network.png\", dpi=200)\n",
    "    plt.close()\n",
    "except Exception as e:\n",
    "    print(\"Network figure skipped:\", e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
